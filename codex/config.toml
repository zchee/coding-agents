#:schema /Users/zchee/src/github.com/zchee/schema/codex.config.schema.json

model = "gpt-5.3-codex"
review_model = "gpt-5.3-codex"
model_context_window = 272000 # default: 272000
project_doc_max_bytes = 46080 # default: 32768
project_doc_fallback_filenames = ["AGENTS.md", "CLAUDE.md", "GEMINI.md", "CRUSH.md"]
js_repl_node_path = "/opt/homebrew/bin/node"
zsh_path = "/opt/homebrew/bin/zsh"
hide_agent_reasoning = false # default
show_raw_agent_reasoning = true
model_reasoning_effort = "xhigh"
model_reasoning_summary = "detailed"
model_verbosity = "high"
model_supports_reasoning_summaries = true
check_for_update_on_startup = false
approval_policy = "never"
sandbox_mode = "danger-full-access"
personality = "none"
# forced_login_method = "chatgpt"
web_search = "live"
suppress_unstable_features_warning = true

# instructions = ""

# Stop being nice, and it's the best thing
# - https://www.reddit.com/r/PromptEngineering/comments/1okppqe/comment/nme6p95
# developer_instructions = """From now on, act as my high-level strategic collaborator — not a cheerleader, not a tyrant. Challenge my assumptions and thinking when needed, but always ground your feedback in real-world context, logic, and practicality. Speak with clarity and candor, but with emotional intelligence — direct, not harsh.
#
# When you disagree, explain why and offer a better-reasoned alternative or a sharper question that moves us forward.
#
# Focus on synthesis and impact — help me see the forest and the path through it. Every response should balance: • Truth — objective analysis without sugar-coating. • Nuance — awareness of constraints, trade-offs, and context. • Action — a prioritized next step or strategic recommendation.
#
# Treat me as an equal partner in the process. The goal is not to win arguments but to produce clarity, traction, and progress."""

[shell_environment_policy]
inherit = "all" # inherit can be "all" (default), "core", or "none"
experimental_use_profile = false

[tui]
notifications = true
animations = true

[tools]
view_image = true

[agents]
max_threads = 16

[memories]
max_raw_memories_for_global = 1024
max_rollout_age_days = 30
max_rollouts_per_startup = 8
min_rollout_idle_hours = 12
phase_1_model = "gpt-5.3-codex-spark"
phase_2_model = "gpt-5.3-codex"

# [skills]

[features]
undo = true
unified_exec = true
shell_zsh_fork = true
js_repl = true
js_repl_tools_only = true
search_tool = true
codex_git_commit = true
runtime_metrics = true
sqlite = true
memory_tool = true
child_agents_md = true
apply_patch_freeform = true
remote_models = true
enable_request_compression = true
multi_agent = true
apps = true
skill_mcp_dependency_install = true
skill_env_var_dependency_prompt = true
steer = true
collaboration_modes = true
personality = true
prevent_idle_sleep = true
responses_websockets = true
responses_websockets_v2 = true

[ghost_snapshot] 
disable_warnings = true

[analytics]
enabled = false

[feedback]
enabled = false

[notice]
hide_full_access_warning = true
hide_gpt5_1_migration_prompt = true
hide_rate_limit_model_nudge = false
hide_world_writable_warning = false

# Model providers / Profiles

[model_providers.xai]
name = "xAI"
base_url = "https://api.x.ai/v1"
env_key = "XAI_API_KEY"

[profiles.xai]
model_provider = "xai"
model = "grok-4-1-fast-reasoning" # model = "grok-code-fast-1"

# TODO(zchee): not working yet.
# [model_providers.gemini]
# name = "Gemini"
# base_url = "http://localhost:5555/v1"
#
# [profiles.gemini]
# model_provider = "gemini"
# model = "gemini-3-pro-preview"

[model_providers.openrouter]
name = "OpenRouter"
base_url = "https://openrouter.ai/api/v1"
env_key = "OPENROUTER_API_KEY"

[profiles.gpt-5-pro]
model_provider = "openrouter"
model = "openai/gpt-5-pro"

[profiles.claude]
model_provider = "openrouter"
model = "anthropic/claude-sonnet-4.5"

# MCP

[mcp_servers.alphaxiv]
enabled = true
url = "https://api.alphaxiv.org/mcp/v1"

[mcp_servers.aws-knowledge]
enabled = false
url = "https://knowledge-mcp.global.api.aws"

[mcp_servers.aws-documentation]
enabled = false
command = "uvx"
args = ["awslabs.aws-documentation-mcp-server@latest"]

# https://github.com/zchee/mcp-servers/tree/main/apple-docs
[mcp_servers.apple-docs]
enabled = false
command = "uv"
args = ["run", "--directory=/Users/zchee/go/src/github.com/zchee/mcp-servers/apple-docs", "apple-docs"]

[mcp_servers.filesystem]
enabled = true
command = "rust-mcp-filesystem"
args = ["--allow-write", "/Users/zchee", "/opt/local"]

[mcp_servers.gemini-google-search]
enabled = true
command = "npx"
args = ["-y", "mcp-gemini-google-search@latest"]
env_vars = ["GOOGLE_API_KEY", "GEMINI_MODEL"]
env = { "GEMINI_API_KEY" = "GOOGLE_API_KEY" }

# TODO(zchee): investigate exec_policy
# [mcp_servers.execshell]
# enabled = true
# command = "node"
# args = ["/opt/local/var/bun/install/global/node_modules/@openai/codex-shell-tool-mcp/bin/mcp-server.js"]

[mcp_servers.context7]
enabled = true
url = "https://mcp.context7.com/mcp"
env_http_headers = { "CONTEXT7_API_KEY" = "CONTEXT7_API_KEY" }

# https://github.com/zchee/mcp-sequential-thinking
[mcp_servers.sequential-thinking]
enabled = true
command = "mcp-sequential-thinking"

# https://developers.google.com/knowledge/mcp
[mcp_servers.google-developer-knowledge-dev]
enabled = false
url = "https://developerknowledge.googleapis.com/mcp"
env_http_headers = { "X-Goog-Api-Key" = "GAUDIY_GCP_DEVELOPERKNOWLEDGE_API_KEY_DEV" }

[mcp_servers.google-developer-knowledge-test]
enabled = false
url = "https://developerknowledge.googleapis.com/mcp"
env_http_headers = { "X-Goog-Api-Key" = "GAUDIY_GCP_DEVELOPERKNOWLEDGE_API_KEY_TEST" }

[mcp_servers.google-developer-knowledge-prod]
enabled = false
url = "https://developerknowledge.googleapis.com/mcp"
env_http_headers = { "X-Goog-Api-Key" = "GAUDIY_GCP_DEVELOPERKNOWLEDGE_API_KEY_PROD" }

## LSP
[mcp_servers.bash-language-server]
enabled = false
command = "mcp-language-server"
args = ["--workspace", ".", "--lsp", "bash-language-server", "--", "start"]

[mcp_servers.gopls]
enabled = true
command = "gopls"
args = ["mcp"]

[mcp_servers.basedpyright]
enabled = false
command = "mcp-language-server"
args = ["--workspace", ".", "--lsp", "basedpyright-langserver", "--", "--stdio"]

[mcp_servers.ts_ls]
enabled = false
command = "mcp-language-server"
args = ["--workspace", ".", "--lsp", "typescript-language-server", "--", "--stdio"]

[mcp_servers.lua_ls]
enabled = false
command = "mcp-language-server"
args = ["--workspace", ".", "--lsp", "lua-language-server"]

[mcp_servers.neocmakelsp]
enabled = false
command = "mcp-language-server"
args = ["--workspace", ".", "--lsp", "neocmakelsp", "--", "stdio"]

[mcp_servers.zls]
enabled = false
command = "mcp-language-server"
args = ["--workspace", ".", "--lsp", "zls"]

[mcp_servers.clangd]
enabled = false
command = "mcp-language-server"
args = [
  "--workspace",
  ".",
  "--lsp",
  "/opt/llvm/clangd/bin/clangd",
  "--",
  "--background-index",
  "--background-index-priority=normal",
  "--all-scopes-completion",
  "--background-index",
  "--background-index-priority=normal",
  "--clang-tidy",
  "--completion-parse=auto",
  "--completion-style=detailed",
  "--experimental-modules-support",
  "--function-arg-placeholders=1",
  "--header-insertion=iwyu",
  "--header-insertion-decorators",
  "--import-insertions",
  "--include-ineligible-results",
  "--limit-references=0",
  "--limit-results=0",
  "--ranking-model=heuristics",
  "--rename-file-limit=1000",
  "--enable-config",
  "-j=16",
  "--parse-forwarding-functions",
  "--pch-storage=memory",
  "--use-dirty-headers",
  "--input-style=standard",
]
